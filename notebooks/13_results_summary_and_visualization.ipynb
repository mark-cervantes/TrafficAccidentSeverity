{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "924e4869",
   "metadata": {},
   "source": [
    "# 13: Results Summary, Visualization, and Model Synthesis\n",
    "\n",
    "This notebook focuses on loading the consolidated model performance data, creating comparative visualizations, plotting ROC curves, and performing a synthesis of model suitability.\n",
    "\n",
    "**PRD References:** 2.2.1 (Model Comparison), 3.1.5 (Model Performance Metrics), 9.2 (Summary Report), 9.3 (Performance Logging), 10.5 (Hyperparameter Logging), 10.6 (Model Suitability), FR3 (Model Training & Tuning), FR5 (Model Synthesis)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7ce404",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a95c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json # For loading hyperparameters string\n",
    "\n",
    "# Scikit-learn utilities for ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# Ensure src directory is in Python path\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "from modeling_utils import load_model # To load models for ROC curve plotting\n",
    "\n",
    "# Configuration\n",
    "REPORTS_DIR = Path.cwd().parent / 'reports'\n",
    "PERFORMANCE_EXCEL_FILE = REPORTS_DIR / 'model_performance_summary.xlsx'\n",
    "FIGURES_DIR = REPORTS_DIR / 'figures' / 'summary_visualizations'\n",
    "MODELS_DIR = Path.cwd().parent / 'models' # For loading models\n",
    "DATA_DIR = Path.cwd().parent / 'data'\n",
    "PROCESSED_DATA_FILE = DATA_DIR / 'processed' / 'preprocessed_data.csv'\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2003942",
   "metadata": {},
   "source": [
    "## 2. Load Model Performance Data from Excel\n",
    "\n",
    "We will load the 'Model_Summaries' sheet which should contain the final evaluation metrics for all models, and potentially CV trial data for robustness analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6ada62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary_df = None\n",
    "all_cv_trials_data = {}\n",
    "try:\n",
    "    all_sheets = pd.read_excel(PERFORMANCE_EXCEL_FILE, sheet_name=None, engine='openpyxl')\n",
    "    if 'Model_Summaries' in all_sheets:\n",
    "        model_summary_df = all_sheets['Model_Summaries']\n",
    "        if 'Test_F1' in model_summary_df.columns:\n",
    "            model_summary_df = model_summary_df.sort_values(by='Test_F1', ascending=False).reset_index(drop=True)\n",
    "        print(\"Model performance summary loaded successfully:\")\n",
    "        display(model_summary_df[['Model', 'Test_Precision', 'Test_Recall', 'Test_F1', 'Test_ROC_AUC', 'Training_Time_Seconds', 'CV_Best_F1_Score']])\n",
    "        \n",
    "        # Load CV trial sheets for robustness analysis\n",
    "        for sheet_name, df_sheet in all_sheets.items():\n",
    "            if sheet_name != 'Model_Summaries' and '_CV_Trials' in sheet_name:\n",
    "                model_name_from_sheet = sheet_name.replace('_CV_Trials', '')\n",
    "                all_cv_trials_data[model_name_from_sheet] = df_sheet\n",
    "                print(f\"Loaded CV trials for {model_name_from_sheet}\")\n",
    "    else:\n",
    "        print(f\"Error: 'Model_Summaries' sheet not found in {PERFORMANCE_EXCEL_FILE}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Performance Excel file not found at {PERFORMANCE_EXCEL_FILE}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the Excel file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd38bb",
   "metadata": {},
   "source": [
    "## 3. Comparative Visualizations of Model Performance Metrics\n",
    "\n",
    "Bar charts comparing key metrics (F1, ROC-AUC, Precision, Recall) across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9478a9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_summary_df is not None and not model_summary_df.empty:\n",
    "    metrics_to_plot = ['Test_F1', 'Test_ROC_AUC', 'Test_Precision', 'Test_Recall', 'Training_Time_Seconds']\n",
    "    num_models = len(model_summary_df['Model'].unique())\n",
    "\n",
    "    for metric in metrics_to_plot:\n",
    "        if metric in model_summary_df.columns:\n",
    "            plt.figure(figsize=(max(10, num_models * 0.9), 7))\n",
    "            # For Training_Time_Seconds, a different sorting might be preferred or no sorting by this metric itself\n",
    "            plot_order = model_summary_df['Model'] if metric != 'Training_Time_Seconds' else model_summary_df.sort_values(by='Training_Time_Seconds')['Model']\n",
    "            sns.barplot(x='Model', y=metric, data=model_summary_df, palette='viridis', order=plot_order)\n",
    "            plt.title(f'Comparison of Models by {metric}')\n",
    "            plt.ylabel(metric)\n",
    "            plt.xlabel('Model')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.tight_layout()\n",
    "            plot_filename = FIGURES_DIR / f'model_comparison_{metric.lower()}.png'\n",
    "            plt.savefig(plot_filename)\n",
    "            print(f\"Saved plot to {plot_filename}\")\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Metric '{metric}' not found in model_summary_df. Skipping plot.\")\n",
    "else:\n",
    "    print(\"Model performance data not loaded. Skipping comparative visualizations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f20b3b",
   "metadata": {},
   "source": [
    "## 4. Plotting ROC Curves\n",
    "\n",
    "To plot ROC curves, we need the true labels (`y_test`) and the predicted probabilities for the positive class (`y_test_prob`) for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4502e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_roc, y_test_roc = None, None\n",
    "feature_names_roc = None\n",
    "\n",
    "try:\n",
    "    df_roc = pd.read_csv(PROCESSED_DATA_FILE)\n",
    "    if 'SEVERITY' in df_roc.columns:\n",
    "        y_roc_full = df_roc['SEVERITY']\n",
    "        X_roc_full = df_roc.drop('SEVERITY', axis=1)\n",
    "        feature_names_roc = X_roc_full.columns.tolist()\n",
    "        \n",
    "        _, X_test_roc, _, y_test_roc = train_test_split(\n",
    "            X_roc_full, y_roc_full, test_size=0.2, random_state=RANDOM_STATE, stratify=y_roc_full\n",
    "        )\n",
    "        print(f\"Test data for ROC curves loaded: X_test_roc shape {X_test_roc.shape}, y_test_roc shape {y_test_roc.shape}\")\n",
    "    else:\n",
    "        print(\"Error: Target column 'SEVERITY' not found in preprocessed data for ROC.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Processed data file not found at {PROCESSED_DATA_FILE} for ROC.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading data for ROC curves: {e}\")\n",
    "\n",
    "if X_test_roc is not None and y_test_roc is not None and model_summary_df is not None:\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    \n",
    "    model_filenames_map = {\n",
    "        model_name: f\"{model_name.lower().replace(' ', '_').replace('-', '_')}_best_model{'.pkl' if model_name == 'Logistic Regression' else '.joblib'}\" \n",
    "        for model_name in model_summary_df['Model'].unique()\n",
    "    }\n",
    "\n",
    "    for model_name_iter in model_summary_df['Model']:\n",
    "        if model_name_iter in model_filenames_map:\n",
    "            model_filename = model_filenames_map[model_name_iter]\n",
    "            model_path = MODELS_DIR / model_filename\n",
    "            print(f\"Loading {model_name_iter} from {model_path} for ROC curve...\")\n",
    "            model = load_model(model_path, model_name=model_name_iter)\n",
    "            \n",
    "            if model and hasattr(model, 'predict_proba'):\n",
    "                try:\n",
    "                    X_test_ordered = X_test_roc[feature_names_roc] \n",
    "                    y_test_prob = model.predict_proba(X_test_ordered)[:, 1]\n",
    "                    fpr, tpr, _ = roc_curve(y_test_roc, y_test_prob)\n",
    "                    roc_auc_val = auc(fpr, tpr)\n",
    "                    plt.plot(fpr, tpr, label=f'{model_name_iter} (AUC = {roc_auc_val:.3f})', linewidth=2)\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not generate ROC curve for {model_name_iter}: {e}\")\n",
    "            elif model:\n",
    "                print(f\"Model {model_name_iter} does not have predict_proba method. Skipping ROC curve.\")\n",
    "        else:\n",
    "            print(f\"Filename not defined for {model_name_iter}. Skipping ROC curve.\")\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Chance') \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=14)\n",
    "    plt.ylabel('True Positive Rate', fontsize=14)\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curves', fontsize=16)\n",
    "    plt.legend(loc='lower right', fontsize=10)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    roc_plot_filename = FIGURES_DIR / 'roc_curves_comparison.png'\n",
    "    plt.savefig(roc_plot_filename)\n",
    "    print(f\"Saved ROC curves plot to {roc_plot_filename}\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Test data or model summary not available. Skipping ROC curve plotting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b508e559",
   "metadata": {},
   "source": [
    "## 5. Model Synthesis and Suitability Analysis\n",
    "\n",
    "This section analyzes the comprehensive results from the Excel sheet and visualizations to synthesize findings and recommend the most suitable model(s).\n",
    "\n",
    "**Key Considerations:**\n",
    "1.  **Predictive Performance:** Primarily Test F1-score and Test ROC-AUC. Also consider Precision and Recall based on project goals (e.g., is minimizing false negatives or false positives more critical for 'severe' accidents?).\n",
    "2.  **Interpretability:** How easy is it to understand the model's predictions? (e.g., Logistic Regression coefficients, Decision Tree rules vs. black-box nature of ensembles like Random Forest or XGBoost before SHAP).\n",
    "3.  **Training Time/Computational Cost:** From `Training_Time_Seconds` in `Model_Summaries`. Relevant for retraining frequency or resource constraints.\n",
    "4.  **Robustness to Hyperparameters:** Insights from CV trial sheets (e.g., `*_CV_Trials`). How sensitive is performance to parameter choices? A model with stable performance across a range of parameters might be preferred.\n",
    "5.  **Complexity:** Simpler models are often preferred if performance is comparable.\n",
    "6.  **Feature Importance Consistency:** Do different top models highlight similar important features? (Insights from Notebook 12)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bfef94",
   "metadata": {},
   "source": [
    "### 5.1. Analysis of Predictive Performance\n",
    "\n",
    "*(This section will be filled based on the actual `model_summary_df` output and visualizations generated above.)*\n",
    "\n",
    "- **Overall Best Performers:** Identify models with the highest Test F1-score and Test ROC-AUC.\n",
    "- **Precision vs. Recall Trade-off:** Discuss if any models show a particular strength in Precision or Recall and why this might be relevant.\n",
    "- **Overfitting:** Compare Train vs. Test scores. Significant gaps might indicate overfitting, even if test scores are high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe17e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_summary_df is not None:\n",
    "    print(\"--- Predictive Performance Analysis ---\")\n",
    "    # Display relevant columns for performance comparison\n",
    "    performance_cols = ['Model', 'Test_F1', 'Test_ROC_AUC', 'Test_Precision', 'Test_Recall', 'Train_F1', 'Train_ROC_AUC', 'CV_Best_F1_Score']\n",
    "    display(model_summary_df[performance_cols])\n",
    "    \n",
    "    # Example: Identify top 3 models by Test_F1\n",
    "    top_f1_models = model_summary_df.nlargest(3, 'Test_F1')['Model'].tolist()\n",
    "    print(f\"\\nTop 3 models by Test F1-score: {top_f1_models}\")\n",
    "    \n",
    "    # Example: Check for overfitting (simple check: difference > 0.05)\n",
    "    if 'Train_F1' in model_summary_df.columns and 'Test_F1' in model_summary_df.columns:\n",
    "        model_summary_df['F1_Overfit_Diff'] = model_summary_df['Train_F1'] - model_summary_df['Test_F1']\n",
    "        print(\"\\nModels with F1 difference (Train - Test) > 0.05:\")\n",
    "        display(model_summary_df[model_summary_df['F1_Overfit_Diff'] > 0.05][['Model', 'Train_F1', 'Test_F1', 'F1_Overfit_Diff']])\n",
    "else:\n",
    "    print(\"Model summary data not available for predictive performance analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b24013",
   "metadata": {},
   "source": [
    "### 5.2. Analysis of Interpretability\n",
    "\n",
    "- **Logistic Regression / Decision Tree:** Highly interpretable. Coefficients / tree structure can be directly examined.\n",
    "- **Ensemble Models (Random Forest, XGBoost, LightGBM, Bagging, AdaBoost):** Less directly interpretable but feature importance scores (model-based, SHAP) provide insights.\n",
    "- Discuss which models offer better balance if interpretability is a key requirement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d913d6b",
   "metadata": {},
   "source": [
    "### 5.3. Analysis of Training Time and Computational Cost\n",
    "\n",
    "*(Refer to the 'Training_Time_Seconds' column in `model_summary_df` and the bar chart generated above.)*\n",
    "\n",
    "- Identify models with significantly longer/shorter training times.\n",
    "- Discuss if training time is a major constraint for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19941ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_summary_df is not None and 'Training_Time_Seconds' in model_summary_df.columns:\n",
    "    print(\"\\n--- Training Time Analysis ---\")\n",
    "    display(model_summary_df[['Model', 'Training_Time_Seconds']].sort_values(by='Training_Time_Seconds'))\n",
    "else:\n",
    "    print(\"Training time data not available in model summary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea0bfcb",
   "metadata": {},
   "source": [
    "### 5.4. Analysis of Robustness to Hyperparameters\n",
    "\n",
    "*(This requires examining the `*_CV_Trials` sheets in the Excel file. For each model, look at the variance in CV scores (e.g., `CV_F1_Std`) or how performance changes across different hyperparameter sets.)*\n",
    "\n",
    "- **Example for one model (e.g., XGBoost):**\n",
    "  - Load `XGBoost_CV_Trials` sheet.\n",
    "  - Analyze `CV_F1_Mean` and `CV_F1_Std` for different `Hyperparameter_Set_Tried`.\n",
    "  - A model whose performance is stable across a reasonable range of hyperparameters is more robust.\n",
    "- General comments on which models appeared more/less sensitive during tuning (if observable from CV summary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba30cc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_cv_trials_data:\n",
    "    print(\"\\n--- Hyperparameter Robustness Analysis (Example for one model) ---\")\n",
    "    # Example: Analyze XGBoost CV trials if available\n",
    "    sample_model_for_cv_analysis = 'XGBoost' # Or any other model with CV trials logged\n",
    "    if sample_model_for_cv_analysis in all_cv_trials_data:\n",
    "        cv_df = all_cv_trials_data[sample_model_for_cv_analysis]\n",
    "        print(f\"\\nCV Trial Data for {sample_model_for_cv_analysis}:\")\n",
    "        # Show a few key columns from the CV trials\n",
    "        cv_cols_to_show = ['Hyperparameter_Set_Tried', 'CV_F1_Mean', 'CV_F1_Std', 'CV_Rank_F1']\n",
    "        # Ensure columns exist before trying to display them\n",
    "        cv_cols_to_show = [col for col in cv_cols_to_show if col in cv_df.columns]\n",
    "        display(cv_df[cv_cols_to_show].sort_values(by='CV_Rank_F1' if 'CV_Rank_F1' in cv_df.columns else 'CV_F1_Mean', ascending= True if 'CV_Rank_F1' in cv_df.columns else False).head(10))\n",
    "        \n",
    "        if 'CV_F1_Std' in cv_df.columns:\n",
    "            print(f\"\\nAverage CV F1 Standard Deviation for {sample_model_for_cv_analysis}: {cv_df['CV_F1_Std'].mean():.4f}\")\n",
    "            print(f\"Max CV F1 Standard Deviation for {sample_model_for_cv_analysis}: {cv_df['CV_F1_Std'].max():.4f}\")\n",
    "    else:\n",
    "        print(f\"CV trial data for {sample_model_for_cv_analysis} not found.\")\n",
    "else:\n",
    "    print(\"No CV trial data loaded for robustness analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b054a4c",
   "metadata": {},
   "source": [
    "### 5.5. Overall Model Recommendation\n",
    "\n",
    "*(Synthesize all the above points to make a final recommendation.)*\n",
    "\n",
    "- **Primary Recommendation:** Based on the primary evaluation metric (e.g., Test F1-score) and considering other factors like ROC-AUC, training time, and interpretability.\n",
    "- **Alternative(s):** If there are close contenders or models that excel in specific aspects (e.g., best interpretability with acceptable performance).\n",
    "- **Justification:** Clearly state the reasons for the recommendation, highlighting the trade-offs considered.\n",
    "\n",
    "**Example Structure for Recommendation:**\n",
    "\"Considering the balance between predictive performance (Test F1: [value], Test ROC-AUC: [value]), reasonable training time ([value] seconds), and insights from feature importance analysis, the **[Recommended Model Name]** is recommended as the primary model for forecasting severe EDSA accidents.\n",
    "\n",
    "While [Alternative Model 1] showed slightly [better/worse performance in X metric], its [drawback, e.g., longer training time / lower interpretability] makes it a secondary option. [Alternative Model 2] offers excellent interpretability but with a notable drop in predictive power.\n",
    "\n",
    "The choice of [Recommended Model Name] is supported by its [mention specific strengths, e.g., robustness observed in CV trials, ability to capture complex interactions as seen in SHAP if applicable]. Further steps could involve [e.g., more extensive hyperparameter tuning for this model or deploying it for operational testing].\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673c98c8",
   "metadata": {},
   "source": [
    "## 6. Final Conclusion for this Notebook\n",
    "\n",
    "This notebook has loaded the performance summary of all trained models, generated comparative visualizations for key metrics, and plotted ROC curves. A detailed model synthesis and suitability analysis was performed, considering various factors including predictive accuracy, interpretability, computational cost, and robustness. Based on this comprehensive analysis, a recommendation for the most suitable model(s) for the task has been formulated. These findings and analyses will form a core part of the final project report."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic-severity",
   "language": "python",
   "name": "trafficseverity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
