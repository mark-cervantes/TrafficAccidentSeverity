{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10: Bagging Classifier Model\n",
    "\n",
    "This notebook focuses on developing, tuning, and evaluating a Bagging Classifier for predicting severe traffic accidents. We will likely use Decision Trees as base estimators.\n",
    "\n",
    "**PRD References:** 3.1.5.5 (Bagging), FR3 (Model Training & Tuning), 9.1 (Jupyter Notebooks), 9.3 (Performance Logging), 10.5 (Hyperparameter Logging)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Ensure src directory is in Python path\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier # Base estimator\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Custom utilities\n",
    "from modeling_utils import compute_classification_metrics, append_performance_record\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = Path.cwd().parent / 'data'\n",
    "PROCESSED_DATA_FILE = DATA_DIR / 'processed' / 'preprocessed_data.csv'\n",
    "MODELS_DIR = Path.cwd().parent / 'models'\n",
    "REPORTS_DIR = Path.cwd().parent / 'reports'\n",
    "PERFORMANCE_EXCEL_FILE = REPORTS_DIR / 'model_performance_summary.xlsx'\n",
    "RANDOM_STATE = 42\n",
    "MODEL_NAME = 'BaggingClassifier'\n",
    "CV_SHEET_NAME = f'{MODEL_NAME}_CV_Trials'\n",
    "MODEL_FILENAME = f'{MODEL_NAME.lower().replace(\" \", \"_\")}_best_model.joblib'\n",
    "\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load data from: /home/cmark/Projects/TrafficAccidentSeverity/data/processed/preprocessed_data.csv\n",
      "Data loaded successfully: (22072, 44)\n",
      "Features shape: (22072, 43), Target shape: (22072,)\n",
      "Target distribution:\n",
      "SEVERITY\n",
      "0    0.931859\n",
      "1    0.068141\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Attempting to load data from: {PROCESSED_DATA_FILE}\")\n",
    "try:\n",
    "    df = pd.read_csv(PROCESSED_DATA_FILE)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Processed data file not found at {PROCESSED_DATA_FILE}\")\n",
    "    print(\"Please ensure '02_data_preprocessing.ipynb' has been run successfully.\")\n",
    "    df = None\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the data: {e}\")\n",
    "    df = None\n",
    "\n",
    "if df is not None:\n",
    "    print(f\"Data loaded successfully: {df.shape}\")\n",
    "    if 'SEVERITY' not in df.columns:\n",
    "        print(\"Error: Target column 'SEVERITY' not found in the dataframe.\")\n",
    "        X, y = None, None\n",
    "    else:\n",
    "        X = df.drop('SEVERITY', axis=1)\n",
    "        y = df['SEVERITY']\n",
    "        print(f\"Features shape: {X.shape}, Target shape: {y.shape}\")\n",
    "        print(f\"Target distribution:\\n{y.value_counts(normalize=True)}\")\n",
    "else:\n",
    "    X, y = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (17657, 43), y_train shape: (17657,)\n",
      "X_test shape: (4415, 43), y_test shape: (4415,)\n",
      "NaNs in X_train: 0\n",
      "NaNs in X_test: 0\n"
     ]
    }
   ],
   "source": [
    "if X is not None and y is not None:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "    # Basic check for NaNs after split\n",
    "    print(f\"NaNs in X_train: {X_train.isnull().sum().sum()}\")\n",
    "    print(f\"NaNs in X_test: {X_test.isnull().sum().sum()}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping train-test split as data was not loaded properly.\")\n",
    "    X_train, X_test, y_train, y_test = None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handle Class Imbalance (via base estimator or BaggingClassifier params)\n",
    "\n",
    "- BaggingClassifier itself doesn't directly have class_weight.\n",
    "- We can pass a base_estimator that supports it (like DecisionTreeClassifier with class_weight='balanced')\n",
    "- or use techniques like SMOTE on the training data before fitting BaggingClassifier.\n",
    "- For simplicity and consistency with other models, we'll explore `class_weight` in the base Decision Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Definition and Hyperparameter Tuning (GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GridSearchCV for BaggingClassifier...\n",
      "Fitting 2 folds for each of 48 candidates, totalling 96 fits\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=5, max_features=1.0, max_samples=0.7, n_estimators=50; total time=   3.1s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=5, max_features=0.7, max_samples=0.7, n_estimators=50; total time=   3.5s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=5, max_features=0.7, max_samples=0.7, n_estimators=50; total time=   4.0s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=5, max_features=0.7, max_samples=1.0, n_estimators=50; total time=   4.2s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=5, max_features=1.0, max_samples=0.7, n_estimators=50; total time=   4.3s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=5, max_features=0.7, max_samples=1.0, n_estimators=50; total time=   4.5s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=5, max_features=0.7, max_samples=0.7, n_estimators=100; total time=   6.1s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=5, max_features=0.7, max_samples=0.7, n_estimators=100; total time=   6.1s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=5, max_features=1.0, max_samples=0.7, n_estimators=100; total time=   6.7s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=5, max_features=1.0, max_samples=0.7, n_estimators=100; total time=   7.0s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=5, max_features=0.7, max_samples=1.0, n_estimators=100; total time=   7.1s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=5, max_features=1.0, max_samples=1.0, n_estimators=50; total time=   3.4s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=5, max_features=0.7, max_samples=1.0, n_estimators=100; total time=   7.8s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=10, max_features=0.7, max_samples=0.7, n_estimators=50; total time=   4.0s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=10, max_features=0.7, max_samples=0.7, n_estimators=50; total time=   3.7s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=5, max_features=1.0, max_samples=1.0, n_estimators=50; total time=   5.0s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=10, max_features=0.7, max_samples=1.0, n_estimators=50; total time=   4.1s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=5, max_features=1.0, max_samples=1.0, n_estimators=100; total time=   7.9s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=10, max_features=0.7, max_samples=1.0, n_estimators=50; total time=   5.5s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=5, max_features=1.0, max_samples=1.0, n_estimators=100; total time=   9.2s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=10, max_features=0.7, max_samples=0.7, n_estimators=100; total time=   8.5s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=10, max_features=1.0, max_samples=0.7, n_estimators=50; total time=   7.0s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=10, max_features=1.0, max_samples=0.7, n_estimators=50; total time=   7.2s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=10, max_features=0.7, max_samples=0.7, n_estimators=100; total time=   8.8s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=10, max_features=0.7, max_samples=1.0, n_estimators=100; total time=   9.1s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=10, max_features=1.0, max_samples=1.0, n_estimators=50; total time=   5.4s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=10, max_features=1.0, max_samples=1.0, n_estimators=50; total time=   4.8s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=10, max_features=1.0, max_samples=0.7, n_estimators=100; total time=   9.1s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=None, max_features=0.7, max_samples=0.7, n_estimators=50; total time=   3.9s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=10, max_features=1.0, max_samples=0.7, n_estimators=100; total time=  11.4s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=10, max_features=0.7, max_samples=1.0, n_estimators=100; total time=  12.7s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=None, max_features=0.7, max_samples=0.7, n_estimators=50; total time=   6.4s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=None, max_features=0.7, max_samples=1.0, n_estimators=50; total time=   5.5s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=None, max_features=0.7, max_samples=1.0, n_estimators=50; total time=   6.3s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=None, max_features=0.7, max_samples=0.7, n_estimators=100; total time=   8.3s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=None, max_features=0.7, max_samples=1.0, n_estimators=100; total time=   6.8s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=None, max_features=1.0, max_samples=0.7, n_estimators=50; total time=   5.8s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=None, max_features=0.7, max_samples=0.7, n_estimators=100; total time=   9.3s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=10, max_features=1.0, max_samples=1.0, n_estimators=100; total time=  11.7s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=10, max_features=1.0, max_samples=1.0, n_estimators=100; total time=  13.8s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=5, max_features=0.7, max_samples=0.7, n_estimators=50; total time=   3.6s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=5, max_features=0.7, max_samples=0.7, n_estimators=50; total time=   3.1s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=None, max_features=1.0, max_samples=0.7, n_estimators=50; total time=   9.2s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=None, max_features=0.7, max_samples=1.0, n_estimators=100; total time=  11.4s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=None, max_features=1.0, max_samples=0.7, n_estimators=100; total time=   9.4s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=None, max_features=1.0, max_samples=0.7, n_estimators=100; total time=   8.3s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=5, max_features=0.7, max_samples=0.7, n_estimators=100; total time=   5.1s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=5, max_features=0.7, max_samples=1.0, n_estimators=50; total time=   2.8s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=5, max_features=0.7, max_samples=1.0, n_estimators=50; total time=   2.7s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=None, max_features=1.0, max_samples=1.0, n_estimators=50; total time=   7.9s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=None, max_features=1.0, max_samples=1.0, n_estimators=50; total time=   8.7s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=None, max_features=1.0, max_samples=1.0, n_estimators=100; total time=   8.5s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=5, max_features=0.7, max_samples=0.7, n_estimators=100; total time=   5.7s\n",
      "[CV] END estimator__class_weight=balanced, estimator__max_depth=None, max_features=1.0, max_samples=1.0, n_estimators=100; total time=   9.1s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=5, max_features=1.0, max_samples=0.7, n_estimators=50; total time=   3.7s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=5, max_features=1.0, max_samples=0.7, n_estimators=50; total time=   4.2s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=5, max_features=0.7, max_samples=1.0, n_estimators=100; total time=   5.0s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=5, max_features=0.7, max_samples=1.0, n_estimators=100; total time=   5.3s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=5, max_features=1.0, max_samples=1.0, n_estimators=50; total time=   4.1s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=5, max_features=1.0, max_samples=1.0, n_estimators=50; total time=   4.3s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=10, max_features=0.7, max_samples=0.7, n_estimators=50; total time=   3.3s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=5, max_features=1.0, max_samples=0.7, n_estimators=100; total time=   5.6s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=10, max_features=0.7, max_samples=0.7, n_estimators=50; total time=   3.7s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=5, max_features=1.0, max_samples=1.0, n_estimators=100; total time=   5.8s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=5, max_features=1.0, max_samples=0.7, n_estimators=100; total time=   6.9s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=5, max_features=1.0, max_samples=1.0, n_estimators=100; total time=   6.8s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=10, max_features=0.7, max_samples=1.0, n_estimators=50; total time=   4.7s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=10, max_features=0.7, max_samples=1.0, n_estimators=50; total time=   5.5s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=10, max_features=0.7, max_samples=0.7, n_estimators=100; total time=   7.1s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=10, max_features=1.0, max_samples=0.7, n_estimators=50; total time=   5.2s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=10, max_features=1.0, max_samples=0.7, n_estimators=50; total time=   6.0s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=10, max_features=0.7, max_samples=0.7, n_estimators=100; total time=   8.1s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=10, max_features=0.7, max_samples=1.0, n_estimators=100; total time=   6.5s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=10, max_features=0.7, max_samples=1.0, n_estimators=100; total time=   7.5s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=10, max_features=1.0, max_samples=1.0, n_estimators=50; total time=   5.3s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=10, max_features=1.0, max_samples=1.0, n_estimators=50; total time=   4.4s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=10, max_features=1.0, max_samples=0.7, n_estimators=100; total time=   8.1s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=None, max_features=0.7, max_samples=0.7, n_estimators=50; total time=   5.4s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=10, max_features=1.0, max_samples=0.7, n_estimators=100; total time=   9.6s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=None, max_features=0.7, max_samples=0.7, n_estimators=50; total time=   6.4s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=10, max_features=1.0, max_samples=1.0, n_estimators=100; total time=   8.1s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=None, max_features=0.7, max_samples=1.0, n_estimators=50; total time=   6.0s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=None, max_features=0.7, max_samples=1.0, n_estimators=50; total time=   6.8s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=None, max_features=1.0, max_samples=0.7, n_estimators=50; total time=   5.8s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=None, max_features=0.7, max_samples=0.7, n_estimators=100; total time=   9.1s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=10, max_features=1.0, max_samples=1.0, n_estimators=100; total time=  11.3s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=None, max_features=0.7, max_samples=0.7, n_estimators=100; total time=   9.5s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=None, max_features=1.0, max_samples=0.7, n_estimators=50; total time=   6.0s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=None, max_features=0.7, max_samples=1.0, n_estimators=100; total time=  10.0s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=None, max_features=0.7, max_samples=1.0, n_estimators=100; total time=   9.7s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=None, max_features=1.0, max_samples=1.0, n_estimators=50; total time=   6.2s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=None, max_features=1.0, max_samples=1.0, n_estimators=50; total time=   6.6s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=None, max_features=1.0, max_samples=0.7, n_estimators=100; total time=   7.8s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=None, max_features=1.0, max_samples=0.7, n_estimators=100; total time=   7.4s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=None, max_features=1.0, max_samples=1.0, n_estimators=100; total time=   5.9s\n",
      "[CV] END estimator__class_weight=None, estimator__max_depth=None, max_features=1.0, max_samples=1.0, n_estimators=100; total time=   5.1s\n",
      "GridSearchCV completed in 62.69 seconds.\n",
      "Best F1 score from GridSearchCV: 0.9037\n",
      "Best parameters from GridSearchCV: {'estimator__class_weight': None, 'estimator__max_depth': None, 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "if X_train is not None and y_train is not None:\n",
    "    # Base estimator\n",
    "    base_estimator = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "    bagging_classifier = BaggingClassifier(\n",
    "        estimator=base_estimator, # Changed from base_estimator to estimator\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1 # Use all available cores for Bagging itself\n",
    "    )\n",
    "\n",
    "    # Define a parameter grid\n",
    "    param_grid_fast = {\n",
    "        'n_estimators': [50, 100], # Number of base estimators\n",
    "        'estimator__max_depth': [5, 10, None], # Max depth of base Decision Tree\n",
    "        'estimator__class_weight': ['balanced', None], # Class weight for base Decision Tree\n",
    "        'max_samples': [0.7, 1.0],    # Fraction of samples for training each base estimator\n",
    "        'max_features': [0.7, 1.0]  # Fraction of features for training each base estimator\n",
    "    }\n",
    "    \n",
    "    # A more comprehensive grid for thorough tuning (can be time-consuming)\n",
    "    param_grid_full = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'estimator__max_depth': [5, 10, 20, None],\n",
    "        'estimator__min_samples_split': [2, 5, 10],\n",
    "        'estimator__min_samples_leaf': [1, 2, 4],\n",
    "        'estimator__class_weight': ['balanced', None],\n",
    "        'max_samples': [0.5, 0.7, 1.0],\n",
    "        'max_features': [0.5, 0.7, 1.0],\n",
    "        'bootstrap': [True, False],\n",
    "        'bootstrap_features': [True, False]\n",
    "    }\n",
    "\n",
    "    # Custom ROC AUC scorer to ensure predict_proba is used\n",
    "    def roc_auc_proba_scorer(estimator, X_data, y_true_data):\n",
    "        if hasattr(estimator, \"predict_proba\"):\n",
    "            y_proba = estimator.predict_proba(X_data)[:, 1]\n",
    "            return roc_auc_score(y_true_data, y_proba, average='weighted', multi_class='ovr')\n",
    "        else: # Fallback for estimators without predict_proba (though BaggingClassifier should have it)\n",
    "            return 0.5 # Neutral score\n",
    "\n",
    "    scoring = {\n",
    "        'F1': make_scorer(f1_score, average='weighted'),\n",
    "        'ROC_AUC': roc_auc_proba_scorer,\n",
    "        'Precision': make_scorer(precision_score, average='weighted', zero_division=0),\n",
    "        'Recall': make_scorer(recall_score, average='weighted', zero_division=0)\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=bagging_classifier,\n",
    "        param_grid=param_grid_fast, # Using the faster grid for now\n",
    "        scoring=scoring,\n",
    "        refit='F1',\n",
    "        cv=2, # Reduced CV folds for faster initial run\n",
    "        verbose=2,\n",
    "        n_jobs=-1 # GridSearchCV's n_jobs\n",
    "    )\n",
    "\n",
    "    print(f\"Starting GridSearchCV for {MODEL_NAME}...\")\n",
    "    start_time_grid_search = time.time()\n",
    "    try:\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        end_time_grid_search = time.time()\n",
    "        grid_search_duration = end_time_grid_search - start_time_grid_search\n",
    "        print(f\"GridSearchCV completed in {grid_search_duration:.2f} seconds.\")\n",
    "        print(f\"Best F1 score from GridSearchCV: {grid_search.best_score_:.4f}\")\n",
    "        print(f\"Best parameters from GridSearchCV: {grid_search.best_params_}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during GridSearchCV: {e}\")\n",
    "        grid_search = None\n",
    "        grid_search_duration = 0\n",
    "else:\n",
    "    print(\"Skipping GridSearchCV as training data is not available.\")\n",
    "    grid_search = None\n",
    "    grid_search_duration = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Log Hyperparameter Tuning Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging 48 CV trials to Excel sheet 'BaggingClassifier_CV_Trials'...\n",
      "Info: Sheet 'BaggingClassifier_CV_Trials' not found in /home/cmark/Projects/TrafficAccidentSeverity/reports/model_performance_summary.xlsx. Creating new sheet with common columns.\n",
      "CV trials logging complete.\n"
     ]
    }
   ],
   "source": [
    "if grid_search is not None and hasattr(grid_search, 'cv_results_'):\n",
    "    cv_results = grid_search.cv_results_\n",
    "    print(f\"Logging {len(cv_results['params'])} CV trials to Excel sheet '{CV_SHEET_NAME}'...\")\n",
    "\n",
    "    for i in range(len(cv_results['params'])):\n",
    "        params_tried = cv_results['params'][i]\n",
    "        # Ensure all keys are present, provide default if not (e.g. for different scorers)\n",
    "        record = {\n",
    "            'Model': MODEL_NAME,\n",
    "            'Sheet_Context': 'CV_Trial',\n",
    "            'Hyperparameter_Set_Tried': json.dumps(params_tried),\n",
    "            'CV_F1_Mean': cv_results.get('mean_test_F1', [np.nan]*len(cv_results['params']))[i],\n",
    "            'CV_F1_Std': cv_results.get('std_test_F1', [np.nan]*len(cv_results['params']))[i],\n",
    "            'CV_ROC_AUC_Mean': cv_results.get('mean_test_ROC_AUC', [np.nan]*len(cv_results['params']))[i],\n",
    "            'CV_ROC_AUC_Std': cv_results.get('std_test_ROC_AUC', [np.nan]*len(cv_results['params']))[i],\n",
    "            'CV_Precision_Mean': cv_results.get('mean_test_Precision', [np.nan]*len(cv_results['params']))[i],\n",
    "            'CV_Precision_Std': cv_results.get('std_test_Precision', [np.nan]*len(cv_results['params']))[i],\n",
    "            'CV_Recall_Mean': cv_results.get('mean_test_Recall', [np.nan]*len(cv_results['params']))[i],\n",
    "            'CV_Recall_Std': cv_results.get('std_test_Recall', [np.nan]*len(cv_results['params']))[i],\n",
    "            'CV_Rank_F1': cv_results.get('rank_test_F1', [np.nan]*len(cv_results['params']))[i],\n",
    "            'Fit_Time_Seconds_Mean': cv_results.get('mean_fit_time', [np.nan]*len(cv_results['params']))[i]\n",
    "        }\n",
    "        append_performance_record(PERFORMANCE_EXCEL_FILE, record, sheet_name=CV_SHEET_NAME)\n",
    "    print(\"CV trials logging complete.\")\n",
    "else:\n",
    "    print(\"Skipping CV trials logging as GridSearchCV results are not available or an error occurred.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Model Evaluation and Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best BaggingClassifier Model Performance:\n",
      "Training Set Metrics: {'Precision': 0.9989816882676894, 'Recall': 0.9989805742764909, 'F1': 0.9989770108697928, 'ROC_AUC': np.float64(0.9999988127812636)}\n",
      "Test Set Metrics: {'Precision': 0.8997168030242177, 'Recall': 0.9306908267270668, 'F1': 0.9039773055765735, 'ROC_AUC': np.float64(0.6843987066285289)}\n",
      "Final model performance logged.\n",
      "Best BaggingClassifier model saved to /home/cmark/Projects/TrafficAccidentSeverity/models/baggingclassifier_best_model.joblib\n"
     ]
    }
   ],
   "source": [
    "if grid_search is not None and hasattr(grid_search, 'best_estimator_') and X_train is not None and y_train is not None and X_test is not None and y_test is not None:\n",
    "    best_bagging_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Predictions\n",
    "    y_train_pred = best_bagging_model.predict(X_train)\n",
    "    y_train_prob = best_bagging_model.predict_proba(X_train)[:, 1]\n",
    "    y_test_pred = best_bagging_model.predict(X_test)\n",
    "    y_test_prob = best_bagging_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Compute metrics\n",
    "    train_metrics = compute_classification_metrics(y_train, y_train_pred, y_train_prob)\n",
    "    test_metrics = compute_classification_metrics(y_test, y_test_pred, y_test_prob)\n",
    "\n",
    "    print(f\"Best {MODEL_NAME} Model Performance:\")\n",
    "    print(f\"Training Set Metrics: {train_metrics}\")\n",
    "    print(f\"Test Set Metrics: {test_metrics}\")\n",
    "\n",
    "    # Log final model performance\n",
    "    final_record = {\n",
    "        'Model': MODEL_NAME,\n",
    "        'Sheet_Context': 'Final_Model',\n",
    "        'Selected_Final_Hyperparameters': json.dumps(best_params),\n",
    "        'Training_Time_Seconds': grid_search_duration, # Total GridSearchCV time\n",
    "        'Train_Precision': train_metrics.get('Precision'),\n",
    "        'Train_Recall': train_metrics.get('Recall'),\n",
    "        'Train_F1': train_metrics.get('F1'),\n",
    "        'Train_ROC_AUC': train_metrics.get('ROC_AUC'),\n",
    "        'Test_Precision': test_metrics.get('Precision'),\n",
    "        'Test_Recall': test_metrics.get('Recall'),\n",
    "        'Test_F1': test_metrics.get('F1'),\n",
    "        'Test_ROC_AUC': test_metrics.get('ROC_AUC'),\n",
    "        'CV_Best_F1_Score': grid_search.best_score_ if hasattr(grid_search, 'best_score_') else np.nan\n",
    "    }\n",
    "    append_performance_record(PERFORMANCE_EXCEL_FILE, final_record, sheet_name='Model_Summaries')\n",
    "    print(\"Final model performance logged.\")\n",
    "\n",
    "    # Save the best model\n",
    "    model_save_path = MODELS_DIR / MODEL_FILENAME\n",
    "    try:\n",
    "        joblib.dump(best_bagging_model, model_save_path)\n",
    "        print(f\"Best {MODEL_NAME} model saved to {model_save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model {MODEL_NAME} to {model_save_path}: {e}\")\n",
    "else:\n",
    "    print(\"Skipping final model evaluation, logging, and saving as prerequisites are not met (e.g., data loading error, GridSearchCV error, or test data missing).\")\n",
    "    best_bagging_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing for BaggingClassifier.\n",
      "The Bagging Classifier was successfully trained, tuned, evaluated, and saved.\n",
      "Best parameters found: {'estimator__class_weight': None, 'estimator__max_depth': None, 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 50}\n",
      "Test F1 Score: 0.9040, Test ROC AUC: 0.6844\n",
      "All results, including hyperparameter trials for BaggingClassifier (if run), are logged in '/home/cmark/Projects/TrafficAccidentSeverity/reports/model_performance_summary.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Finished processing for {MODEL_NAME}.\")\n",
    "if 'best_bagging_model' in locals() and best_bagging_model is not None:\n",
    "    print(\"The Bagging Classifier was successfully trained, tuned, evaluated, and saved.\")\n",
    "    print(f\"Best parameters found: {best_params}\")\n",
    "    print(f\"Test F1 Score: {test_metrics.get('F1'):.4f}, Test ROC AUC: {test_metrics.get('ROC_AUC'):.4f}\")\n",
    "else:\n",
    "    print(\"The Bagging Classifier could not be fully processed due to earlier errors (check logs).\")\n",
    "\n",
    "print(f\"All results, including hyperparameter trials for {MODEL_NAME} (if run), are logged in '{PERFORMANCE_EXCEL_FILE}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic-severity",
   "language": "python",
   "name": "trafficseverity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
