{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lgbm-intro-md",
   "metadata": {},
   "source": [
    "# 08: LightGBM Model\n",
    "\n",
    "This notebook focuses on developing, tuning, and evaluating a LightGBM classifier for predicting severe traffic accidents.\n",
    "\n",
    "**PRD References:** 3.1.5.4 (LightGBM), FR3 (Model Training & Tuning), 9.1 (Jupyter Notebooks), 9.3 (Performance Logging), 10.5 (Hyperparameter Logging)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lgbm-setup-md",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "lgbm-setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Ensure src directory is in Python path\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# LightGBM import\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Custom utilities\n",
    "from modeling_utils import compute_classification_metrics, append_performance_record\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = Path.cwd().parent / 'data'\n",
    "PROCESSED_DATA_FILE = DATA_DIR / 'processed' / 'preprocessed_data.csv'\n",
    "MODELS_DIR = Path.cwd().parent / 'models'\n",
    "REPORTS_DIR = Path.cwd().parent / 'reports'\n",
    "PERFORMANCE_EXCEL_FILE = REPORTS_DIR / 'model_performance_summary.xlsx'\n",
    "RANDOM_STATE = 42\n",
    "MODEL_NAME = 'LightGBM'\n",
    "CV_SHEET_NAME = f'{MODEL_NAME}_CV_Trials'\n",
    "MODEL_FILENAME = f'{MODEL_NAME.lower().replace(\" \", \"_\")}_best_model.joblib'\n",
    "\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lgbm-load-data-md",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "Load the preprocessed data. Ensure that the target variable `SEVERITY` and all features are correctly identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "lgbm-load-data-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully: (22072, 44)\n",
      "Features shape: (22072, 43), Target shape: (22072,)\n",
      "Target distribution:\n",
      "SEVERITY\n",
      "0   0.93186\n",
      "1   0.06814\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv(PROCESSED_DATA_FILE)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Processed data file not found at {PROCESSED_DATA_FILE}\")\n",
    "    print(\"Please ensure '02_data_preprocessing.ipynb' has been run successfully.\")\n",
    "    df = None\n",
    "\n",
    "if df is not None:\n",
    "    print(f\"Data loaded successfully: {df.shape}\")\n",
    "    if 'SEVERITY' not in df.columns:\n",
    "        print(\"Error: Target column 'SEVERITY' not found in the dataframe.\")\n",
    "        X, y = None, None\n",
    "    else:\n",
    "        X = df.drop('SEVERITY', axis=1)\n",
    "        y = df['SEVERITY']\n",
    "        print(f\"Features shape: {X.shape}, Target shape: {y.shape}\")\n",
    "        print(f\"Target distribution:\\n{y.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lgbm-split-data-md",
   "metadata": {},
   "source": [
    "## 3. Train-Test Split\n",
    "\n",
    "Split the data into training and testing sets, stratifying by the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "lgbm-split-data-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (17657, 43), y_train shape: (17657,)\n",
      "X_test shape: (4415, 43), y_test shape: (4415,)\n"
     ]
    }
   ],
   "source": [
    "if X is not None and y is not None:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "else:\n",
    "    print(\"Skipping train-test split as data was not loaded properly.\")\n",
    "    X_train, X_test, y_train, y_test = None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lgbm-imbalance-md",
   "metadata": {},
   "source": [
    "## 4. Handle Class Imbalance (Optional)\n",
    "\n",
    "LightGBM has parameters like `class_weight='balanced'` or `is_unbalance=True` / `scale_pos_weight` to handle imbalanced datasets. We will include `class_weight` in our hyperparameter grid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lgbm-tuning-md",
   "metadata": {},
   "source": [
    "## 5. Model Definition and Hyperparameter Tuning (GridSearchCV)\n",
    "\n",
    "We'll use GridSearchCV to find the best hyperparameters for the LightGBM model. A custom scorer for ROC AUC is defined to ensure correct probability handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "lgbm-tuning-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GridSearchCV for LightGBM...\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] END class_weight=balanced, colsample_bytree=0.5, learning_rate=0.01, max_depth=-1, n_estimators=50, num_leaves=31, reg_alpha=0.0, reg_lambda=0.0, subsample=0.5; total time= 3.4min\n",
      "[CV] END class_weight=balanced, colsample_bytree=0.5, learning_rate=0.01, max_depth=-1, n_estimators=50, num_leaves=31, reg_alpha=0.0, reg_lambda=0.0, subsample=0.5; total time= 3.5min\n",
      "[CV] END class_weight=balanced, colsample_bytree=0.5, learning_rate=0.01, max_depth=-1, n_estimators=50, num_leaves=31, reg_alpha=0.0, reg_lambda=0.0, subsample=0.5; total time= 3.5min\n",
      "GridSearchCV completed in 211.29 seconds.\n",
      "Best F1 score from GridSearchCV: 0.7739\n",
      "Best parameters from GridSearchCV: {'class_weight': 'balanced', 'colsample_bytree': 0.5, 'learning_rate': 0.01, 'max_depth': -1, 'n_estimators': 50, 'num_leaves': 31, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 0.5}\n"
     ]
    }
   ],
   "source": [
    "if X_train is not None:\n",
    "    lgbm_classifier = LGBMClassifier(random_state=RANDOM_STATE, verbose=-1) # verbose=-1 to suppress LightGBM's own messages\n",
    "\n",
    "    # Define a standard parameter grid for ideal tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'num_leaves': [20, 31, 50],\n",
    "        'max_depth': [-1, 10, 20], # Default -1 (no limit)\n",
    "        'class_weight': ['balanced', None],\n",
    "        'colsample_bytree': [0.5, 0.7, 1.0],\n",
    "        'subsample': [0.5, 0.7, 1.0],\n",
    "        'reg_alpha': [0.0, 0.1, 1.0],\n",
    "        'reg_lambda': [0.0, 0.1, 1.0]\n",
    "    }\n",
    "    # Define a smaller, more focused parameter grid for faster initial run to finish in couple of seconds\n",
    "    param_grid_fast = {\n",
    "        'n_estimators': [50],\n",
    "        'learning_rate': [0.01],\n",
    "        'num_leaves': [31],\n",
    "        'max_depth': [-1],\n",
    "        'class_weight': ['balanced'],\n",
    "        'colsample_bytree': [0.5],\n",
    "        'subsample': [0.5],\n",
    "        'reg_alpha': [0.0],\n",
    "        'reg_lambda': [0.0]\n",
    "    }\n",
    "\n",
    "    # Custom ROC AUC scorer to ensure predict_proba is used\n",
    "    def roc_auc_proba_scorer(estimator, X_data, y_true_data):\n",
    "        y_proba = estimator.predict_proba(X_data)[:, 1]\n",
    "        return roc_auc_score(y_true_data, y_proba, average='weighted', multi_class='ovr')\n",
    "\n",
    "    scoring = {\n",
    "        'F1': make_scorer(f1_score, average='weighted'),\n",
    "        'ROC_AUC': roc_auc_proba_scorer, # Use the custom callable\n",
    "        'Precision': make_scorer(precision_score, average='weighted', zero_division=0),\n",
    "        'Recall': make_scorer(recall_score, average='weighted', zero_division=0)\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=lgbm_classifier,\n",
    "        param_grid=param_grid_fast,\n",
    "        scoring=scoring,\n",
    "        refit='F1', # Refit the best model using F1 score\n",
    "        cv=3,       # Number of cross-validation folds (3 for quicker run)\n",
    "        verbose=2,\n",
    "        n_jobs=-1   # Use all available cores\n",
    "    )\n",
    "\n",
    "    print(f\"Starting GridSearchCV for {MODEL_NAME}...\")\n",
    "    start_time_grid_search = time.time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    end_time_grid_search = time.time()\n",
    "    grid_search_duration = end_time_grid_search - start_time_grid_search\n",
    "    print(f\"GridSearchCV completed in {grid_search_duration:.2f} seconds.\")\n",
    "\n",
    "    print(f\"Best F1 score from GridSearchCV: {grid_search.best_score_:.4f}\")\n",
    "    print(f\"Best parameters from GridSearchCV: {grid_search.best_params_}\")\n",
    "else:\n",
    "    print(\"Skipping GridSearchCV as training data is not available.\")\n",
    "    grid_search = None\n",
    "    grid_search_duration = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lgbm-log-cv-md",
   "metadata": {},
   "source": [
    "## 6. Log Hyperparameter Tuning Experiments\n",
    "\n",
    "Log each hyperparameter combination tried by GridSearchCV and its performance to the Excel sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "lgbm-log-cv-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging 1 CV trials to Excel sheet 'LightGBM_CV_Trials'...\n",
      "Info: Sheet 'LightGBM_CV_Trials' not found in /home/cmark/Projects/TrafficAccidentSeverity/reports/model_performance_summary.xlsx. Creating new sheet with common columns.\n",
      "CV trials logging complete.\n"
     ]
    }
   ],
   "source": [
    "if grid_search is not None and hasattr(grid_search, 'cv_results_'):\n",
    "    cv_results = grid_search.cv_results_\n",
    "    print(f\"Logging {len(cv_results['params'])} CV trials to Excel sheet '{CV_SHEET_NAME}'...\")\n",
    "\n",
    "    for i in range(len(cv_results['params'])):\n",
    "        params_tried = cv_results['params'][i]\n",
    "        record = {\n",
    "            'Model': MODEL_NAME,\n",
    "            'Sheet_Context': 'CV_Trial',\n",
    "            'Hyperparameter_Set_Tried': json.dumps(params_tried),\n",
    "            'CV_F1_Mean': cv_results.get('mean_test_F1', [np.nan]*len(cv_results['params']))[i],\n",
    "            'CV_F1_Std': cv_results.get('std_test_F1', [np.nan]*len(cv_results['params']))[i],\n",
    "            'CV_ROC_AUC_Mean': cv_results.get('mean_test_ROC_AUC', [np.nan]*len(cv_results['params']))[i],\n",
    "            'CV_ROC_AUC_Std': cv_results.get('std_test_ROC_AUC', [np.nan]*len(cv_results['params']))[i],\n",
    "            'CV_Precision_Mean': cv_results.get('mean_test_Precision', [np.nan]*len(cv_results['params']))[i],\n",
    "            'CV_Precision_Std': cv_results.get('std_test_Precision', [np.nan]*len(cv_results['params']))[i],\n",
    "            'CV_Recall_Mean': cv_results.get('mean_test_Recall', [np.nan]*len(cv_results['params']))[i],\n",
    "            'CV_Recall_Std': cv_results.get('std_test_Recall', [np.nan]*len(cv_results['params']))[i],\n",
    "            'CV_Rank_F1': cv_results.get('rank_test_F1', [np.nan]*len(cv_results['params']))[i],\n",
    "            'Fit_Time_Seconds_Mean': cv_results.get('mean_fit_time', [np.nan]*len(cv_results['params']))[i]\n",
    "        }\n",
    "        append_performance_record(PERFORMANCE_EXCEL_FILE, record, sheet_name=CV_SHEET_NAME)\n",
    "    print(\"CV trials logging complete.\")\n",
    "else:\n",
    "    print(\"Skipping CV trials logging as GridSearchCV results are not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lgbm-eval-log-md",
   "metadata": {},
   "source": [
    "## 7. Best Model Evaluation and Logging\n",
    "\n",
    "Evaluate the best model found by GridSearchCV on the training and test sets, then log its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "lgbm-eval-log-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LightGBM Model Performance:\n",
      "Training Set Metrics: {'Precision': 0.9160981459716704, 'Recall': 0.7148439712295407, 'F1': 0.7849300300244804, 'ROC_AUC': np.float64(0.7912181632139821)}\n",
      "Test Set Metrics: {'Precision': 0.9026139500891773, 'Recall': 0.6892412231030578, 'F1': 0.7658782480243982, 'ROC_AUC': np.float64(0.7135015836048046)}\n",
      "Final model performance logged.\n",
      "Best LightGBM model saved to /home/cmark/Projects/TrafficAccidentSeverity/models/lightgbm_best_model.joblib\n"
     ]
    }
   ],
   "source": [
    "if grid_search is not None and hasattr(grid_search, 'best_estimator_') and X_train is not None:\n",
    "    best_lgbm_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Predictions\n",
    "    y_train_pred = best_lgbm_model.predict(X_train)\n",
    "    y_train_prob = best_lgbm_model.predict_proba(X_train)[:, 1]\n",
    "    y_test_pred = best_lgbm_model.predict(X_test)\n",
    "    y_test_prob = best_lgbm_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Compute metrics\n",
    "    train_metrics = compute_classification_metrics(y_train, y_train_pred, y_train_prob)\n",
    "    test_metrics = compute_classification_metrics(y_test, y_test_pred, y_test_prob)\n",
    "\n",
    "    print(f\"Best {MODEL_NAME} Model Performance:\")\n",
    "    print(f\"Training Set Metrics: {train_metrics}\")\n",
    "    print(f\"Test Set Metrics: {test_metrics}\")\n",
    "\n",
    "    # Log final model performance\n",
    "    final_record = {\n",
    "        'Model': MODEL_NAME,\n",
    "        'Sheet_Context': 'Final_Model',\n",
    "        'Selected_Final_Hyperparameters': json.dumps(best_params),\n",
    "        'Training_Time_Seconds': grid_search_duration, # Total GridSearchCV time as proxy\n",
    "        'Train_Precision': train_metrics.get('Precision'),\n",
    "        'Train_Recall': train_metrics.get('Recall'),\n",
    "        'Train_F1': train_metrics.get('F1'),\n",
    "        'Train_ROC_AUC': train_metrics.get('ROC_AUC'),\n",
    "        'Test_Precision': test_metrics.get('Precision'),\n",
    "        'Test_Recall': test_metrics.get('Recall'),\n",
    "        'Test_F1': test_metrics.get('F1'),\n",
    "        'Test_ROC_AUC': test_metrics.get('ROC_AUC'),\n",
    "        'CV_Best_F1_Score': grid_search.best_score_\n",
    "    }\n",
    "    append_performance_record(PERFORMANCE_EXCEL_FILE, final_record, sheet_name='Model_Summaries')\n",
    "    print(\"Final model performance logged.\")\n",
    "\n",
    "    # Save the best model\n",
    "    model_save_path = MODELS_DIR / MODEL_FILENAME\n",
    "    joblib.dump(best_lgbm_model, model_save_path)\n",
    "    print(f\"Best {MODEL_NAME} model saved to {model_save_path}\")\n",
    "else:\n",
    "    print(\"Skipping final model evaluation, logging, and saving as prerequisites are not met.\")\n",
    "    best_lgbm_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lgbm-conclusion-md",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "This notebook implemented and tuned a LightGBM classifier. The hyperparameter tuning process was logged, and the best performing model was evaluated and saved. Its performance metrics are recorded in the summary Excel sheet for comparison with other models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic-severity",
   "language": "python",
   "name": "trafficseverity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
